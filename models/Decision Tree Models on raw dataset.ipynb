{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avazu CTR Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Import, clean, engineer features, hashing\n",
    "2) Intro to boosting\n",
    "3) Run a baseline model-no custom parameters\n",
    "4) Add custom hyperparameters to model to reduce overfitting\n",
    "5) Run CV to improve results and find best params\n",
    "6) Run model with the improved params we have found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (roc_auc_score, mean_absolute_error, \n",
    "mean_squared_error, average_precision_score, confusion_matrix,\n",
    "classification_report)\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ignore Warning Messages'''\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set style to GGplot for asthetics'''\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/Pierre/Desktop/repos/BidTime/data/samples/avazu_sample_300k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    df[c]=df[c].apply(str)\n",
    "    le=preprocessing.LabelEncoder().fit(df[c])\n",
    "    df[c] =le.transform(df[c])\n",
    "    pd.to_numeric(df[c]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).values\n",
    "Y = df['click'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100000\n",
    "X_train = X[:n_train]\n",
    "Y_train = Y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "Y_test = Y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_enc = enc.fit_transform(X_train)\n",
    "X_test_enc = enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "DT = DecisionTreeClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 100000\n",
      "ROC of AUC on testing set: 0.717\n",
      "Mean Absolute Error on testing set: 0.255\n",
      "Root Mean Squared Error on testing set: 0.017\n",
      "Average Precision Recall Score on testing set: 0.333\n"
     ]
    }
   ],
   "source": [
    "print('Training samples: {0}'.format(n_train))\n",
    "print('ROC of AUC on testing set: {1:.3f}'.format(n_train, roc_auc_score(Y_test, pred_LR)))\n",
    "print('Mean Absolute Error on testing set: {1:.3f}'.format(n_train, mean_absolute_error(Y_test, pred_LR)))\n",
    "print('Root Mean Squared Error on testing set: {1:.3f}'.format(n_train, (mean_squared_error(Y_test, pred_LR)**2)))\n",
    "print('Average Precision Recall Score on testing set: {1:.3f}'.format(n_train, (average_precision_score(Y_test, pred_LR))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163348,   2535],\n",
       "       [ 31407,   2710]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_pred_LR = [round(value) for value in pred_LR]\n",
    "confusion_matrix(Y_test, rounded_pred_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Click       0.84      0.98      0.91    165883\n",
      "       Click       0.52      0.08      0.14     34117\n",
      "\n",
      "   micro avg       0.83      0.83      0.83    200000\n",
      "   macro avg       0.68      0.53      0.52    200000\n",
      "weighted avg       0.78      0.83      0.77    200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['No Click','Click']\n",
    "cr_pred_LR = LR.predict(X_test_enc)\n",
    "print(str(classification_report(Y_test, cr_pred_LR,target_names=target_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = SGDClassifier(loss='log', random_state=42)\n",
    "SGD.fit(X_train_enc.toarray(), Y_train)\n",
    "\n",
    "pred_SGD = SGD.predict_proba(X_test_enc.toarray())[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 100000\n",
      "ROC of AUC on testing set: 0.717\n",
      "Mean Absolute Error on testing set: 0.260\n",
      "Root Mean Squared Error on testing set: 0.017\n",
      "Average Precision Recall Score on testing set: 0.335\n"
     ]
    }
   ],
   "source": [
    "print('Training samples: {0}'.format(n_train))\n",
    "print('ROC of AUC on testing set: {1:.3f}'.format(n_train, roc_auc_score(Y_test, pred_SGD)))\n",
    "print('Mean Absolute Error on testing set: {1:.3f}'.format(n_train, mean_absolute_error(Y_test, pred_SGD)))\n",
    "print('Root Mean Squared Error on testing set: {1:.3f}'.format(n_train, (mean_squared_error(Y_test, pred_SGD)**2)))\n",
    "print('Average Precision Recall Score on testing set: {1:.3f}'.format(n_train, (average_precision_score(Y_test, pred_SGD))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163073,   2810],\n",
       "       [ 31117,   3000]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_pred_SGD = [round(value) for value in pred_SGD]\n",
    "confusion_matrix(Y_test, rounded_pred_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Click       0.84      0.98      0.91    165883\n",
      "       Click       0.52      0.09      0.15     34117\n",
      "\n",
      "   micro avg       0.83      0.83      0.83    200000\n",
      "   macro avg       0.68      0.54      0.53    200000\n",
      "weighted avg       0.78      0.83      0.78    200000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr_pred_SGD = SGD.predict(X_test_enc)\n",
    "print(str(classification_report(Y_test, cr_pred_SGD, target_names=target_names)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
