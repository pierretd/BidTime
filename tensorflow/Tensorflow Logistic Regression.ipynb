{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 300000\n",
    "df = pd.read_csv(\"train.gz\", nrows=n_rows, compression=\"infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.columns:\n",
    "    df[c]=df[c].apply(str)\n",
    "    le=preprocessing.LabelEncoder().fit(df[c])\n",
    "    df[c] =le.transform(df[c])\n",
    "    pd.to_numeric(df[c]).astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['click', 'id', 'hour', 'device_id', 'device_ip'], axis=1).values\n",
    "Y = df['click'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(n_rows * 0.9)\n",
    "X_train = X[:n_train]\n",
    "Y_train = Y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "Y_test = Y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X_train_enc = enc.fit_transform(X_train)\n",
    "X_test_enc = enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = int(X_train_enc.toarray().shape[1])\n",
    "learning_rate = 0.001\n",
    "n_iter = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and Target placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, n_features])\n",
    "y = tf.placeholder(tf.float32, shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Build the logistic regression model\n",
    "W = tf.Variable(tf.zeros([n_features, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.add(tf.matmul(x, W), b)[:, 0]\n",
    "pred = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "auc = tf.metrics.auc(tf.cast(y, tf.int64), pred)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables\n",
    "init_vars = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(indices):\n",
    "    np.random.shuffle(indices)\n",
    "    for batch_i in range(int(n_train / batch_size)):\n",
    "        batch_index = indices[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "        yield X_train_enc[batch_index], Y_train[batch_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training loss: 0.464202\n",
      "Iteration 2, training loss: 0.414743\n",
      "Iteration 3, training loss: 0.409080\n",
      "Iteration 4, training loss: 0.405974\n",
      "Iteration 5, training loss: 0.403799\n",
      "Iteration 6, training loss: 0.402161\n",
      "Iteration 7, training loss: 0.400820\n",
      "Iteration 8, training loss: 0.399711\n",
      "Iteration 9, training loss: 0.398795\n",
      "Iteration 10, training loss: 0.397979\n",
      "Iteration 11, training loss: 0.397265\n",
      "Iteration 12, training loss: 0.396620\n",
      "Iteration 13, training loss: 0.396049\n",
      "Iteration 14, training loss: 0.395520\n",
      "Iteration 15, training loss: 0.395071\n",
      "Iteration 16, training loss: 0.394638\n",
      "Iteration 17, training loss: 0.394227\n",
      "Iteration 18, training loss: 0.393846\n",
      "Iteration 19, training loss: 0.393504\n",
      "Iteration 20, training loss: 0.393150\n",
      "Iteration 21, training loss: 0.392868\n",
      "Iteration 22, training loss: 0.392578\n",
      "Iteration 23, training loss: 0.392318\n",
      "Iteration 24, training loss: 0.392053\n",
      "Iteration 25, training loss: 0.391800\n",
      "Iteration 26, training loss: 0.391579\n",
      "Iteration 27, training loss: 0.391355\n",
      "Iteration 28, training loss: 0.391151\n",
      "Iteration 29, training loss: 0.390970\n",
      "Iteration 30, training loss: 0.390792\n",
      "Iteration 31, training loss: 0.390582\n",
      "Iteration 32, training loss: 0.390397\n",
      "Iteration 33, training loss: 0.390272\n",
      "Iteration 34, training loss: 0.390071\n",
      "Iteration 35, training loss: 0.389907\n",
      "Iteration 36, training loss: 0.389792\n",
      "Iteration 37, training loss: 0.389656\n",
      "Iteration 38, training loss: 0.389483\n",
      "Iteration 39, training loss: 0.389363\n",
      "Iteration 40, training loss: 0.389249\n",
      "Iteration 41, training loss: 0.389139\n",
      "Iteration 42, training loss: 0.389020\n",
      "Iteration 43, training loss: 0.388904\n",
      "Iteration 44, training loss: 0.388801\n",
      "Iteration 45, training loss: 0.388667\n",
      "Iteration 46, training loss: 0.388590\n",
      "Iteration 47, training loss: 0.388492\n",
      "Iteration 48, training loss: 0.388380\n",
      "Iteration 49, training loss: 0.388288\n",
      "Iteration 50, training loss: 0.388191\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, n_iter+1):\n",
    "    avg_cost = 0.\n",
    "    for X_batch, Y_batch in gen_batch(indices):\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: X_batch.toarray(), y: Y_batch})\n",
    "        avg_cost += c / int(n_train / batch_size)\n",
    "    print('Iteration %i, training loss: %f' % (i, avg_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of ROC on testing set: 0.76679647\n"
     ]
    }
   ],
   "source": [
    "auc_test = sess.run(auc, feed_dict={x: X_test_enc.toarray(), y: Y_test})\n",
    "print(\"AUC of ROC on testing set:\", auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
